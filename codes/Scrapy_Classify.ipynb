{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "postal-cambodia",
   "metadata": {},
   "source": [
    "# Library import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "continuous-franchise",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import logging\n",
    "import numpy as np\n",
    "import re\n",
    "import tqdm\n",
    "from threading import *\n",
    "import queue\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radical-discharge",
   "metadata": {},
   "source": [
    "# Target links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "elder-virtue",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=336277\n",
    "end = 339000\n",
    "link_list=[]\n",
    "for i in range(start,end):\n",
    "    link_list.append('http://www.newsmth.net/nForum/article/Intern/'+str(i)+'?ajax' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-confidence",
   "metadata": {},
   "source": [
    "# Scrapy Funtion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-administrator",
   "metadata": {},
   "source": [
    "## Process extends Thread Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "supreme-exclusive",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShuimuSpider(Thread):\n",
    "    def __init__(self,url,q):\n",
    "        super(ShuimuSpider,self).__init__()\n",
    "        self.url=url\n",
    "        self.q=q\n",
    "        \n",
    "    def run(self):\n",
    "        self.parse_page()\n",
    "        \n",
    "    def send_request(self,url):\n",
    "        try:\n",
    "            html = requests.get(url,timeout=2)\n",
    "        except Exception as e:\n",
    "            print(\"fail at %s\" % url)\n",
    "        else:\n",
    "            return html\n",
    "    def parse_page(self):\n",
    "        response = self.send_request(self.url)\n",
    "        if response.status_code==200:\n",
    "            response_bs=response.content\n",
    "            try:\n",
    "                bs = BeautifulSoup(response_bs,'html.parser')\n",
    "\n",
    "\n",
    "                text=''\n",
    "                store_data={\n",
    "                    'time':[],\n",
    "                    'title':[],\n",
    "                    'detail':[]\n",
    "                }\n",
    "                text = bs.p.text\n",
    "                # .p 对应html中的<p>, 观察帖子不难发现，几乎所有的招聘正文都在帖子的<p>内\n",
    "                # .text 对应输出文本\n",
    "\n",
    "                #分别通过特殊字符串的位置给标题、时间、正文定位\n",
    "                #position for title:\n",
    "                Title_Position1=text.find('标  题: ')+len('标  题: ')\n",
    "                Title_Position2=text.find('  发信站: 水木社区')\n",
    "                Title = text[Title_Position1:Title_Position2]\n",
    "                #text[x:y]进行切片\n",
    "\n",
    "                #position for time:\n",
    "                Time_Position1=text.find('  发信站: 水木社区')+len('  发信站: 水木社区')\n",
    "                Time_Position2=text.find('), 站内')+1\n",
    "                Time=text[Time_Position1:Time_Position2]\n",
    "\n",
    "                #position for detail\n",
    "                Detail_Position1=text.find('), 站内')+len('), 站内')+4\n",
    "                Detail_Position2=text.find('※ 来源:·水木社区')\n",
    "                Detail=text[Detail_Position1:Detail_Position2]\n",
    "\n",
    "                store_data['time'].append(Time)\n",
    "                store_data['title'].append(Title)\n",
    "                store_data['detail'].append(Detail)\n",
    "                #字典赋值\n",
    "                #运行成功，index序列加一\n",
    "\n",
    "                self.q.put(store_data)\n",
    "            except:\n",
    "                return\n",
    "        else:\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-singapore",
   "metadata": {},
   "source": [
    "## Scrapy function with multi-threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "twelve-ecuador",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fast_Scrapy(x,y,store_data):\n",
    "    start = time.time()\n",
    "    q= queue.Queue(0)\n",
    "\n",
    "    Thread_list=[]\n",
    "    for url in link_list[x:y]:\n",
    "        p = ShuimuSpider(url,q)\n",
    "        p.start()\n",
    "        Thread_list.append(p)\n",
    "\n",
    "    for i in Thread_list:\n",
    "        i.join()\n",
    "\n",
    "    while not q.empty():\n",
    "        data = q.get()\n",
    "\n",
    "        store_data['time'].append(data['time'])\n",
    "        store_data['title'].append(data['title'])\n",
    "        store_data['detail'].append(data['detail'])\n",
    "    end = time.time()\n",
    "    print('当前Process + Queue多线程爬虫的总时间为：', end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-cooking",
   "metadata": {},
   "source": [
    "## Scrapy RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "junior-candle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scrapy():\n",
    "    store_data={\n",
    "        'time':[],\n",
    "        'title':[],\n",
    "        'detail':[]\n",
    "    }\n",
    "    if end-start>150:\n",
    "        period = np.arange(0,end-start,150)\n",
    "\n",
    "        for i in tqdm.tqdm(period):\n",
    "            Fast_Scrapy(i,i+150,store_data)\n",
    "        Fast_Scrapy(i,end,store_data)\n",
    "    else:\n",
    "        Fast_Scrapy(start,end,store_data)\n",
    "        \n",
    "    df = pd.DataFrame(store_data)\n",
    "    df.to_csv('../datasets/'+str(start)+'-'+str(end)+'.csv')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efficient-cloud",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▎                                                                              | 1/19 [00:03<01:05,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 3.642998456954956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|████████▋                                                                          | 2/19 [00:06<00:58,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 3.28900408744812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█████████████                                                                      | 3/19 [00:10<00:53,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 3.292998790740967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|█████████████████▍                                                                 | 4/19 [00:13<00:49,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 3.238001823425293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|█████████████████████▊                                                             | 5/19 [00:16<00:45,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 3.1750032901763916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|██████████████████████████▏                                                        | 6/19 [00:19<00:41,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 3.1099977493286133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|██████████████████████████████▌                                                    | 7/19 [00:23<00:38,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 3.302004098892212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|██████████████████████████████████▉                                                | 8/19 [00:26<00:36,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 3.3380353450775146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|███████████████████████████████████████▎                                           | 9/19 [00:29<00:32,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 3.15999698638916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|███████████████████████████████████████████▏                                      | 10/19 [00:32<00:28,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 3.166415214538574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|███████████████████████████████████████████████▍                                  | 11/19 [00:35<00:25,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 3.1499993801116943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|███████████████████████████████████████████████████▊                              | 12/19 [00:39<00:22,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 3.322000026702881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|████████████████████████████████████████████████████████                          | 13/19 [00:42<00:19,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 3.203997850418091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|████████████████████████████████████████████████████████████▍                     | 14/19 [00:45<00:16,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 3.2929999828338623\n",
      "fail at http://www.newsmth.net/nForum/article/Intern/338465?ajax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-2195:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python39\\lib\\threading.py\", line 954, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"<ipython-input-3-19f1ba8616d8>\", line 8, in run\n",
      "  File \"<ipython-input-3-19f1ba8616d8>\", line 19, in parse_page\n",
      "AttributeError: 'NoneType' object has no attribute 'status_code'\n",
      " 79%|████████████████████████████████████████████████████████████████▋                 | 15/19 [00:49<00:13,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 3.4619998931884766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|█████████████████████████████████████████████████████████████████████             | 16/19 [00:52<00:09,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 3.3259990215301514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|█████████████████████████████████████████████████████████████████████████▎        | 17/19 [00:55<00:06,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 3.1929972171783447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████████████████████████████████████████████████████████████████████████▋    | 18/19 [00:58<00:03,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 3.1789627075195312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [00:59<00:00,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 0.629037618637085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 0.6779940128326416\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = Scrapy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-workplace",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "possible-forward",
   "metadata": {},
   "source": [
    "## Remove '[]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "descending-administrator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove '[]'\n",
    "for col in ['time','title','detail']:\n",
    "    df[col]=df[col].apply(lambda x:x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-berlin",
   "metadata": {},
   "source": [
    "## Transfer to pd.datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "higher-texas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer to pd.datetime\n",
    "df['time']=(df['time'].str[2:12]+df['time'].str[21:-1])\n",
    "df['time']=pd.to_datetime(df['time'],errors='coerce')\n",
    "df['time']=df['time'].fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-financing",
   "metadata": {},
   "source": [
    "## remove data with extremely short title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "reflected-laundry",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['title'].str.len()>5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-commerce",
   "metadata": {},
   "source": [
    "## 去重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "british-practice",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['title'].duplicated()]\n",
    "df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-sitting",
   "metadata": {},
   "source": [
    "## Remove datas without a delivering email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "systematic-median",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy =df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "official-arctic",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy=df_copy[~df_copy['title'].str.contains('个人提供')]\n",
    "df_copy=df_copy[~df_copy['title'].str.contains('博后')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "indie-attraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,4}'\n",
    "regex = re.compile(pattern,flags=re.IGNORECASE)\n",
    "df['Email']='NaN'\n",
    "for i in range(len(df)):\n",
    "    try:\n",
    "        df.iloc[i,3]=regex.findall(df['detail'][i])\n",
    "    except:\n",
    "        continue\n",
    "df.replace(to_replace='NaN',value = np.nan,regex=True,inplace =True)\n",
    "df.dropna(axis=0,how='any',inplace =True)\n",
    "df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-classics",
   "metadata": {},
   "source": [
    "## output as text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ahead-norman",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(df):\n",
    "    text=''\n",
    "    for i in range(len(df)):\n",
    "        text += str(df.index[i])+' '\n",
    "        text += df['title'][i]+'\\n'\n",
    "        text += df['detail'][i]+'\\n'\n",
    "        #text += df['Email'][i]+'\\n\\n\\n'\n",
    "    \n",
    "    with open('2021-05-06.txt','w',encoding='utf-8') as f:\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "distributed-italic",
   "metadata": {},
   "outputs": [],
   "source": [
    "output(df_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-discrimination",
   "metadata": {},
   "source": [
    "# Classify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-penguin",
   "metadata": {},
   "source": [
    "## Load Classify Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "medical-flooring",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "clf_model=joblib.load('../models/Classification_model_5_n_estimator=100_pca=100.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrapped-jacket",
   "metadata": {},
   "source": [
    "## Predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "medium-mailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "mark = ['人力资源', '其他', '券商投行基金', '前端&测试', '投资咨询实习生', '数据分析挖掘', '研发开发',\n",
    "       '算法实习', '运营实习', '量化交易']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "appropriate-accountability",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba_fast\n",
    "\n",
    "stopwords = pd.read_csv('../stopwords/baidu_stopwords.txt',\n",
    "                        quoting=3,sep='\\t',names=['stopword'],encoding='utf-8')\n",
    "stopwords = stopwords['stopword'].values\n",
    "\n",
    "def pre_input(input):\n",
    "    sentences=[]\n",
    "    segs=jieba_fast.cut(input,cut_all=False)\n",
    "    segs = list(filter(lambda x:x.strip(),segs))\n",
    "    segs=list(filter(lambda x:((x not in stopwords) and x != 'xa0' ),segs))\n",
    "    sentences.append(str([x for x in segs]))\n",
    "    return sentences\n",
    "\n",
    "def predict_df(x):\n",
    "    return mark[clf_model.predict(pre_input(x))[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-mailing",
   "metadata": {},
   "source": [
    "## predict for dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "metropolitan-administration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "req = re.compile('(职责|内容|要求)(.|\\n)*')\n",
    "df['requirement']=np.zeros(len(df))\n",
    "def find_req(x):\n",
    "    try: \n",
    "        re.search(req,x).span()\n",
    "        return x[re.search(req,x).span()[0] : re.search(req,x).span()[1]]\n",
    "    except:\n",
    "        return ''\n",
    "df['requirement']=df['detail'].apply(find_req)\n",
    "df = df[df['requirement']!='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "available-slovakia",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category']=np.zeros(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "sharing-maria",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "standard-surface",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 50.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['category']=(df['title']+df['requirement']).apply(lambda x:predict_df(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "rocky-format",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a6b7eee9fd29>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m70\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'category'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df[50:60][['title','category']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-evans",
   "metadata": {},
   "source": [
    "数据集7000   岗位种类少 0.803"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-treasury",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
