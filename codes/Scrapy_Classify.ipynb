{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "postal-cambodia",
   "metadata": {},
   "source": [
    "# Library import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "continuous-franchise",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import logging\n",
    "import numpy as np\n",
    "import re\n",
    "import tqdm\n",
    "from threading import *\n",
    "import queue\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radical-discharge",
   "metadata": {},
   "source": [
    "# Target links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "elder-virtue",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=323000\n",
    "end = 327000\n",
    "link_list=[]\n",
    "for i in range(start,end):\n",
    "    link_list.append('http://www.newsmth.net/nForum/article/Intern/'+str(i)+'?ajax' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-confidence",
   "metadata": {},
   "source": [
    "# Scrapy Funtion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-administrator",
   "metadata": {},
   "source": [
    "## Process extends Thread Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "supreme-exclusive",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShuimuSpider(Thread):\n",
    "    def __init__(self,url,q):\n",
    "        super(ShuimuSpider,self).__init__()\n",
    "        self.url=url\n",
    "        self.q=q\n",
    "        \n",
    "    def run(self):\n",
    "        self.parse_page()\n",
    "        \n",
    "    def send_request(self,url):\n",
    "        try:\n",
    "            html = requests.get(url,timeout=2)\n",
    "        except Exception as e:\n",
    "            print(\"fail at %s\" % url)\n",
    "        else:\n",
    "            return html\n",
    "    def parse_page(self):\n",
    "        response = self.send_request(self.url)\n",
    "        if response.status_code==200:\n",
    "            response_bs=response.content\n",
    "            try:\n",
    "                bs = BeautifulSoup(response_bs,'html.parser')\n",
    "\n",
    "\n",
    "                text=''\n",
    "                store_data={\n",
    "                    'time':[],\n",
    "                    'title':[],\n",
    "                    'detail':[]\n",
    "                }\n",
    "                text = bs.p.text\n",
    "                # .p 对应html中的<p>, 观察帖子不难发现，几乎所有的招聘正文都在帖子的<p>内\n",
    "                # .text 对应输出文本\n",
    "\n",
    "                #分别通过特殊字符串的位置给标题、时间、正文定位\n",
    "                #position for title:\n",
    "                Title_Position1=text.find('标  题: ')+len('标  题: ')\n",
    "                Title_Position2=text.find('  发信站: 水木社区')\n",
    "                Title = text[Title_Position1:Title_Position2]\n",
    "                #text[x:y]进行切片\n",
    "\n",
    "                #position for time:\n",
    "                Time_Position1=text.find('  发信站: 水木社区')+len('  发信站: 水木社区')\n",
    "                Time_Position2=text.find('), 站内')+1\n",
    "                Time=text[Time_Position1:Time_Position2]\n",
    "\n",
    "                #position for detail\n",
    "                Detail_Position1=text.find('), 站内')+len('), 站内')+4\n",
    "                Detail_Position2=text.find('※ 来源:·水木社区')\n",
    "                Detail=text[Detail_Position1:Detail_Position2]\n",
    "\n",
    "                store_data['time'].append(Time)\n",
    "                store_data['title'].append(Title)\n",
    "                store_data['detail'].append(Detail)\n",
    "                #字典赋值\n",
    "                #运行成功，index序列加一\n",
    "\n",
    "                self.q.put(store_data)\n",
    "            except:\n",
    "                return\n",
    "        else:\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-singapore",
   "metadata": {},
   "source": [
    "## Scrapy function with multi-threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "twelve-ecuador",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fast_Scrapy(x,y,store_data):\n",
    "    start = time.time()\n",
    "    q= queue.Queue(0)\n",
    "\n",
    "    Thread_list=[]\n",
    "    for url in link_list[x:y]:\n",
    "        p = ShuimuSpider(url,q)\n",
    "        p.start()\n",
    "        Thread_list.append(p)\n",
    "\n",
    "    for i in Thread_list:\n",
    "        i.join()\n",
    "\n",
    "    while not q.empty():\n",
    "        data = q.get()\n",
    "\n",
    "        store_data['time'].append(data['time'])\n",
    "        store_data['title'].append(data['title'])\n",
    "        store_data['detail'].append(data['detail'])\n",
    "    end = time.time()\n",
    "    print('当前Process + Queue多线程爬虫的总时间为：', end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-cooking",
   "metadata": {},
   "source": [
    "## Scrapy RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "junior-candle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scrapy():\n",
    "    store_data={\n",
    "        'time':[],\n",
    "        'title':[],\n",
    "        'detail':[]\n",
    "    }\n",
    "    if end-start>150:\n",
    "        period = np.arange(0,end-start,150)\n",
    "\n",
    "        for i in tqdm.tqdm(period):\n",
    "            Fast_Scrapy(i,i+150,store_data)\n",
    "        Fast_Scrapy(i,end,store_data)\n",
    "    else:\n",
    "        Fast_Scrapy(start,end,store_data)\n",
    "        \n",
    "    df = pd.DataFrame(store_data)\n",
    "    df.to_csv('../datasets/'+str(start)+'-'+str(end)+'.csv')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efficient-cloud",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███                                                                                | 1/27 [00:03<01:41,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 3.8927433490753174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|██████▏                                                                            | 2/27 [00:07<01:38,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 3.9433536529541016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|█████████▏                                                                         | 3/27 [00:11<01:34,  3.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 3.9539926052093506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|████████████▎                                                                      | 4/27 [00:15<01:28,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 3.7030510902404785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|███████████████▎                                                                   | 5/27 [00:19<01:25,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 3.896127700805664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██████████████████▍                                                                | 6/27 [00:23<01:20,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 3.707023859024048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|█████████████████████▌                                                             | 7/27 [00:26<01:15,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 3.7303946018218994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|████████████████████████▌                                                          | 8/27 [00:30<01:11,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 3.76003098487854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███████████████████████████▋                                                       | 9/27 [00:34<01:05,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 3.3934171199798584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|██████████████████████████████▎                                                   | 10/27 [00:37<01:01,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 3.610386371612549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|█████████████████████████████████▍                                                | 11/27 [00:41<00:58,  3.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 3.717292547225952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|████████████████████████████████████▍                                             | 12/27 [00:44<00:54,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 3.5361101627349854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|███████████████████████████████████████▍                                          | 13/27 [00:48<00:50,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 3.5664255619049072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|██████████████████████████████████████████▌                                       | 14/27 [00:51<00:46,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 3.4989399909973145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|█████████████████████████████████████████████▌                                    | 15/27 [00:55<00:42,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 3.4568564891815186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|████████████████████████████████████████████████▌                                 | 16/27 [00:58<00:38,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 3.524728298187256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|███████████████████████████████████████████████████▋                              | 17/27 [01:02<00:34,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 3.392265558242798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████████████████████████████████████████████████████▋                           | 18/27 [01:05<00:31,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 3.411975383758545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|█████████████████████████████████████████████████████████▋                        | 19/27 [01:09<00:27,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 3.515995979309082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|████████████████████████████████████████████████████████████▋                     | 20/27 [01:13<00:25,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 4.052040100097656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|███████████████████████████████████████████████████████████████▊                  | 21/27 [01:17<00:22,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 3.789680242538452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|██████████████████████████████████████████████████████████████████▊               | 22/27 [01:20<00:18,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 3.662040948867798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|█████████████████████████████████████████████████████████████████████▊            | 23/27 [01:25<00:15,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 4.459753036499023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|████████████████████████████████████████████████████████████████████████▉         | 24/27 [01:29<00:11,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 3.858135461807251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-3715:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python39\\lib\\threading.py\", line 954, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"<ipython-input-3-19f1ba8616d8>\", line 8, in run\n",
      "  File \"<ipython-input-3-19f1ba8616d8>\", line 19, in parse_page\n",
      "AttributeError: 'NoneType' object has no attribute 'status_code'\n",
      " 93%|███████████████████████████████████████████████████████████████████████████▉      | 25/27 [01:33<00:07,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fail at http://www.newsmth.net/nForum/article/Intern/326708?ajax\n",
      "当前Process + Queue多线程爬虫的总时间为： 4.22190260887146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|██████████████████████████████████████████████████████████████████████████████▉   | 26/27 [01:37<00:03,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 3.827993869781494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 27/27 [01:39<00:00,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 2.4944543838500977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Process + Queue多线程爬虫的总时间为： 2.5949952602386475\n",
      "Wall time: 1min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = Scrapy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-workplace",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "possible-forward",
   "metadata": {},
   "source": [
    "## Remove '[]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "descending-administrator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove '[]'\n",
    "for col in ['time','title','detail']:\n",
    "    df[col]=df[col].apply(lambda x:x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-berlin",
   "metadata": {},
   "source": [
    "## Transfer to pd.datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "higher-texas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer to pd.datetime\n",
    "df['time']=(df['time'].str[2:12]+df['time'].str[21:-1])\n",
    "df['time']=pd.to_datetime(df['time'],errors='coerce')\n",
    "df['time']=df['time'].fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-financing",
   "metadata": {},
   "source": [
    "## remove data with extremely short title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "reflected-laundry",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['title'].str.len()>5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-commerce",
   "metadata": {},
   "source": [
    "## 去重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "british-practice",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['title'].duplicated()]\n",
    "df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-sitting",
   "metadata": {},
   "source": [
    "## Remove datas without a delivering email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "indie-attraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,4}'\n",
    "regex = re.compile(pattern,flags=re.IGNORECASE)\n",
    "df['Email']='NaN'\n",
    "for i in range(len(df)):\n",
    "    try:\n",
    "        df.iloc[i,3]=regex.findall(df['detail'][i])\n",
    "    except:\n",
    "        continue\n",
    "df.replace(to_replace='NaN',value = np.nan,regex=True,inplace =True)\n",
    "df.dropna(axis=0,how='any',inplace =True)\n",
    "df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-classics",
   "metadata": {},
   "source": [
    "## output as text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ahead-norman",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(df):\n",
    "    text=''\n",
    "    for i in range(len(df)):\n",
    "        text += str(df.index[i])+' '\n",
    "        text += df['title'][i]+'\\n'\n",
    "        text += df['detail'][i]+'\\n'\n",
    "        text += df['Email'][i]+'\\n\\n\\n'\n",
    "    \n",
    "    with open(str(start)+'-'+str(end)+'.txt','w',encoding='utf-8') as f:\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-discrimination",
   "metadata": {},
   "source": [
    "# Classify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-penguin",
   "metadata": {},
   "source": [
    "## Load Classify Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "medical-flooring",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "clf_model=joblib.load('../models/Classification_model_5_n_estimator=100_pca=100.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrapped-jacket",
   "metadata": {},
   "source": [
    "## Predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "medium-mailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "mark = ['人力资源', '其他', '券商投行基金', '前端&测试', '投资咨询实习生', '数据分析挖掘', '研发开发',\n",
    "       '算法实习', '运营实习', '量化交易']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "appropriate-accountability",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba_fast\n",
    "\n",
    "stopwords = pd.read_csv('../stopwords/baidu_stopwords.txt',\n",
    "                        quoting=3,sep='\\t',names=['stopword'],encoding='utf-8')\n",
    "stopwords = stopwords['stopword'].values\n",
    "\n",
    "def pre_input(input):\n",
    "    sentences=[]\n",
    "    segs=jieba_fast.cut(input,cut_all=False)\n",
    "    segs = list(filter(lambda x:x.strip(),segs))\n",
    "    segs=list(filter(lambda x:((x not in stopwords) and x != 'xa0' ),segs))\n",
    "    sentences.append(str([x for x in segs]))\n",
    "    return sentences\n",
    "\n",
    "def predict_df(x):\n",
    "    return mark[clf_model.predict(pre_input(x))[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-mailing",
   "metadata": {},
   "source": [
    "## predict for dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "metropolitan-administration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "req = re.compile('(职责|内容|要求)(.|\\n)*')\n",
    "df['requirement']=np.zeros(len(df))\n",
    "def find_req(x):\n",
    "    try: \n",
    "        re.search(req,x).span()\n",
    "        return x[re.search(req,x).span()[0] : re.search(req,x).span()[1]]\n",
    "    except:\n",
    "        return ''\n",
    "df['requirement']=df['detail'].apply(find_req)\n",
    "df = df[df['requirement']!='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "available-slovakia",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category']=np.zeros(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "sharing-maria",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "standard-surface",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 50.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['category']=(df['title']+df['requirement']).apply(lambda x:predict_df(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "rocky-format",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>【实习】【搜狐】市场部项目实习生-北京</td>\n",
       "      <td>其他</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>【实习】【银河证券行研】房地产组行业研究实习生招聘</td>\n",
       "      <td>券商投行基金</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>【实习】国信证券-私募直投子公司-国信弘盛-行业研究部-北京</td>\n",
       "      <td>券商投行基金</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>【MSRA】微软亚洲研究院系统组实习生-深度学习工具与系统方向</td>\n",
       "      <td>算法实习</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>【实习】【北京-券商行研】机械行业研究助理招聘</td>\n",
       "      <td>券商投行基金</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>【实习】【微博平台】Java研发（可内推）</td>\n",
       "      <td>研发开发</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>【北京实习】TokenInsight公司招聘金融翻译实习生(懂区块链)</td>\n",
       "      <td>运营实习</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>【北京 实习】TokenInsight招区块链行业研究实习生</td>\n",
       "      <td>投资咨询实习生</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>【光大证券(北京)研究所招聘】</td>\n",
       "      <td>券商投行基金</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>【实习】【百度】数据仓库研发实习生</td>\n",
       "      <td>研发开发</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  title category\n",
       "50                  【实习】【搜狐】市场部项目实习生-北京       其他\n",
       "51            【实习】【银河证券行研】房地产组行业研究实习生招聘   券商投行基金\n",
       "52       【实习】国信证券-私募直投子公司-国信弘盛-行业研究部-北京   券商投行基金\n",
       "53      【MSRA】微软亚洲研究院系统组实习生-深度学习工具与系统方向     算法实习\n",
       "54              【实习】【北京-券商行研】机械行业研究助理招聘   券商投行基金\n",
       "55                【实习】【微博平台】Java研发（可内推）     研发开发\n",
       "56  【北京实习】TokenInsight公司招聘金融翻译实习生(懂区块链)     运营实习\n",
       "57       【北京 实习】TokenInsight招区块链行业研究实习生  投资咨询实习生\n",
       "58                      【光大证券(北京)研究所招聘】   券商投行基金\n",
       "59                    【实习】【百度】数据仓库研发实习生     研发开发"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[50:60][['title','category']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-evans",
   "metadata": {},
   "source": [
    "数据集7000   岗位种类少 0.803"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-treasury",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
